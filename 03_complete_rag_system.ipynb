{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# Session 2.3: Complete RAG System - End-to-End Implementation\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook, we'll build a complete RAG (Retrieval-Augmented Generation) system:\n",
        "- Load and process documents (PDF, text files)\n",
        "- Implement smart chunking strategies\n",
        "- Build a complete RAG pipeline with ChromaDB\n",
        "- Combine retrieval with LLM generation\n",
        "- Compare RAG vs non-RAG answers\n",
        "- Experiment with different configurations\n",
        "\n",
        "**Key Concepts:**\n",
        "- RAG combines retrieval (finding relevant information) with generation (LLM response)\n",
        "- Proper chunking is critical for retrieval quality\n",
        "- RAG reduces hallucinations by grounding answers in documents\n",
        "- Different configurations affect accuracy, speed, and context quality"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "## The RAG Pipeline\n",
        "\n",
        "```\n",
        "INDEXING PHASE (offline, done once):\n",
        "┌─────────────┐      ┌──────────┐      ┌────────────┐      ┌──────────────┐\n",
        "│  Documents  │ ───> │ Chunking │ ───> │ Embeddings │ ───> │  Vector DB   │\n",
        "│ (PDF, txt)  │      │ Strategy │      │ Generation │      │  (ChromaDB)  │\n",
        "└─────────────┘      └──────────┘      └────────────┘      └──────────────┘\n",
        "\n",
        "QUERY PHASE (online, for each query):\n",
        "┌─────────┐      ┌────────────┐      ┌──────────────┐\n",
        "│  Query  │ ───> │  Embed     │ ───> │  Similarity  │\n",
        "└─────────┘      │  Query     │      │    Search    │\n",
        "                 └────────────┘      └──────┬───────┘\n",
        "                                            │\n",
        "                                            ▼\n",
        "                                     ┌──────────────┐\n",
        "                                     │  Top-K       │\n",
        "                                     │  Chunks      │\n",
        "                                     └──────┬───────┘\n",
        "                                            │\n",
        "                                            ▼\n",
        "┌──────────┐      ┌────────────┐      ┌──────────────┐\n",
        "│  Answer  │ <─── │    LLM     │ <─── │   Prompt     │\n",
        "│          │      │ Generation │      │ + Context    │\n",
        "└──────────┘      └────────────┘      └──────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {
        "id": "cell-2"
      },
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {
        "id": "cell-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede52172-52f1-4690-8ce7-c7c557991d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✓ Packages installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install chromadb openai pypdf2 requests -q\n",
        "\n",
        "print(\"✓ Packages installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-4",
      "metadata": {
        "id": "cell-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79dd410e-fdf7-45cc-84a3-8653b79e0ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChromaDB version: 1.3.5\n"
          ]
        }
      ],
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import json\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import requests\n",
        "import PyPDF2\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "print(f\"ChromaDB version: {chromadb.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {
        "id": "cell-5"
      },
      "source": [
        "## Configure OpenRouter API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {
        "id": "cell-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97beb719-5331-4ae6-8276-c29df968cd57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ OpenRouter client configured\n",
            "✓ Embedding model: openai/text-embedding-3-small\n",
            "✓ LLM model: openai/gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "# Add env variables from colab secrets\n",
        "from google.colab import userdata\n",
        "os.environ['OPENROUTER_API_KEY'] = userdata.get('OPENROUTER_API_KEY')\n",
        "\n",
        "OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY', None)\n",
        "\n",
        "if not OPENROUTER_API_KEY:\n",
        "    raise ValueError(\"Please set your OpenRouter API key as an environment variable or directly in the code.\")\n",
        "\n",
        "# Initialize OpenAI client with OpenRouter endpoint\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=OPENROUTER_API_KEY\n",
        ")\n",
        "\n",
        "# Models to use\n",
        "EMBEDDING_MODEL = \"openai/text-embedding-3-small\"\n",
        "LLM_MODEL = \"openai/gpt-4o-mini\"  # Fast and cost-effective for workshop\n",
        "\n",
        "print(f\"✓ OpenRouter client configured\")\n",
        "print(f\"✓ Embedding model: {EMBEDDING_MODEL}\")\n",
        "print(f\"✓ LLM model: {LLM_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "source": [
        "## Part 1: Document Loading and Processing\n",
        "\n",
        "### Why Document Processing Matters\n",
        "\n",
        "Raw documents need preprocessing:\n",
        "- **PDFs**: Extract text from pages\n",
        "- **Cleaning**: Remove extra whitespace, formatting artifacts\n",
        "- **Metadata**: Track source, page numbers for citations\n",
        "- **Quality**: Ensure readable text for embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8",
      "metadata": {
        "id": "cell-8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6201d64-33cb-4a62-e526-e370c56efea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Document loading functions defined\n"
          ]
        }
      ],
      "source": [
        "def load_pdf(file_path: str) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Load PDF and extract text with metadata.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to PDF file\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with text and metadata\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Extract text from all pages\n",
        "        full_text = \"\"\n",
        "        page_texts = []\n",
        "\n",
        "        for page_num, page in enumerate(pdf_reader.pages):\n",
        "            page_text = page.extract_text()\n",
        "            page_texts.append({\n",
        "                \"page_number\": page_num + 1,\n",
        "                \"text\": page_text\n",
        "            })\n",
        "            full_text += page_text + \"\\n\"\n",
        "\n",
        "        return {\n",
        "            \"source\": file_path,\n",
        "            \"full_text\": full_text,\n",
        "            \"page_texts\": page_texts,\n",
        "            \"num_pages\": len(pdf_reader.pages)\n",
        "        }\n",
        "\n",
        "\n",
        "def load_text_file(file_path: str) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Load plain text file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to text file\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with text and metadata\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "        return {\n",
        "            \"source\": file_path,\n",
        "            \"full_text\": text,\n",
        "            \"num_pages\": 1\n",
        "        }\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean extracted text.\n",
        "\n",
        "    Args:\n",
        "        text: Raw text\n",
        "\n",
        "    Returns:\n",
        "        Cleaned text\n",
        "    \"\"\"\n",
        "    # Remove multiple newlines\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "\n",
        "    # Remove page numbers (simple heuristic)\n",
        "    text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "print(\"✓ Document loading functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {
        "id": "cell-9"
      },
      "source": [
        "## Part 2: Chunking Strategies\n",
        "\n",
        "### Why Chunking Matters\n",
        "\n",
        "**The Problem:**\n",
        "- Embedding models have token limits (usually 512-8192 tokens)\n",
        "- Long documents lose semantic focus\n",
        "- Need to split documents into smaller, meaningful pieces\n",
        "\n",
        "**Chunking Strategies:**\n",
        "\n",
        "1. **Fixed Size** - Simple, predictable size\n",
        "2. **Sentence-based** - Maintains semantic coherence\n",
        "3. **Sliding Window with Overlap** - Captures context across boundaries\n",
        "4. **Semantic** - Split on topic changes (more complex)\n",
        "\n",
        "**Key Parameters:**\n",
        "- **Chunk size**: Too small = lacks context; Too large = dilutes relevance\n",
        "- **Sweet spot**: 256-1024 tokens (roughly 200-800 words)\n",
        "- **Overlap**: 10-20% helps capture context at boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10",
      "metadata": {
        "id": "cell-10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5da5b0-1860-4df3-8371-2651bb077dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Chunking functions defined\n"
          ]
        }
      ],
      "source": [
        "def chunk_text_fixed(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "    \"\"\"\n",
        "    Chunk text using fixed word count with overlap.\n",
        "\n",
        "    Args:\n",
        "        text: Text to chunk\n",
        "        chunk_size: Number of words per chunk\n",
        "        overlap: Number of overlapping words between chunks\n",
        "\n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        if len(chunk.strip()) > 0:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_text_sentences(text: str, target_size: int = 500) -> List[str]:\n",
        "    \"\"\"\n",
        "    Chunk text by sentences, grouping to approximate target size.\n",
        "\n",
        "    Args:\n",
        "        text: Text to chunk\n",
        "        target_size: Target number of words per chunk\n",
        "\n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    # Simple sentence splitting (naive, but works for most cases)\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_size = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_size = len(sentence.split())\n",
        "\n",
        "        if current_size + sentence_size > target_size and current_chunk:\n",
        "            # Save current chunk and start new one\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "            current_size = sentence_size\n",
        "        else:\n",
        "            current_chunk.append(sentence)\n",
        "            current_size += sentence_size\n",
        "\n",
        "    # Add last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def analyze_chunks(chunks: List[str]) -> None:\n",
        "    \"\"\"\n",
        "    Analyze chunk statistics.\n",
        "\n",
        "    Args:\n",
        "        chunks: List of text chunks\n",
        "    \"\"\"\n",
        "    word_counts = [len(chunk.split()) for chunk in chunks]\n",
        "    char_counts = [len(chunk) for chunk in chunks]\n",
        "\n",
        "    print(f\"Number of chunks: {len(chunks)}\")\n",
        "    print(f\"\\nWord counts:\")\n",
        "    print(f\"  Min: {min(word_counts)}\")\n",
        "    print(f\"  Max: {max(word_counts)}\")\n",
        "    print(f\"  Average: {sum(word_counts) / len(word_counts):.1f}\")\n",
        "    print(f\"\\nCharacter counts:\")\n",
        "    print(f\"  Min: {min(char_counts)}\")\n",
        "    print(f\"  Max: {max(char_counts)}\")\n",
        "    print(f\"  Average: {sum(char_counts) / len(char_counts):.1f}\")\n",
        "\n",
        "\n",
        "print(\"✓ Chunking functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "source": [
        "## Part 3: Creating Sample Documents\n",
        "\n",
        "For this workshop, we'll create sample documents about AI and machine learning topics.\n",
        "\n",
        "In a real scenario, you would load actual PDFs or documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {
        "id": "cell-12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8d67ecc-65ec-4f42-a0c6-4ee8e71dffbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Created 5 sample documents\n",
            "\n",
            "Document titles:\n",
            "  1. Uvod u umjetnu inteligenciju (osnove)\n",
            "  2. Neuralne mreže i duboko učenje (duboko_ucenje)\n",
            "  3. Veliki jezični modeli (LLM) (llm)\n",
            "  4. RAG sustavi (rag)\n",
            "  5. Primjene umjetne inteligencije (primjene)\n"
          ]
        }
      ],
      "source": [
        "# Sample documents about AI/ML topics (in Croatian for consistency with previous notebooks)\n",
        "sample_documents = [\n",
        "    {\n",
        "        \"title\": \"Uvod u umjetnu inteligenciju\",\n",
        "        \"content\": \"\"\"\n",
        "Umjetna inteligencija (UI) je područje računarstva koje se bavi stvaranjem inteligentnih sustava\n",
        "koji mogu učiti, rasuđivati i rješavati probleme. UI tehnologije uključuju strojno učenje,\n",
        "duboko učenje i obrada prirodnog jezika.\n",
        "\n",
        "Strojno učenje je podskup umjetne inteligencije gdje računala uče iz podataka bez eksplicitnog\n",
        "programiranja. Algoritmi strojnog učenja prepoznaju uzorke u podacima i donose predviđanja ili odluke.\n",
        "\n",
        "Postoje tri glavne vrste strojnog učenja: nadzirano učenje, nenadzirano učenje i učenje\n",
        "potkrepljivanjem. Nadzirano učenje koristi označene podatke za treniranje modela. Nenadzirano\n",
        "učenje pronalazi uzorke u neoznačenim podacima. Učenje potkrepljivanjem uči kroz interakciju s okolinom.\n",
        "\"\"\",\n",
        "        \"category\": \"osnove\",\n",
        "        \"language\": \"hr\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Neuralne mreže i duboko učenje\",\n",
        "        \"content\": \"\"\"\n",
        "Neuralne mreže su računalni modeli inspirirani biološkim neuralnim mrežama. One se sastoje od\n",
        "slojeva međusobno povezanih čvorova (neurona) koji obrađuju informacije.\n",
        "\n",
        "Duboko učenje koristi neuralne mreže s više skrivenih slojeva. Svaki sloj uči sve kompleksnije\n",
        "značajke iz podataka. Na primjer, u prepoznavanju slika, prvi sloj može naučiti rubove,\n",
        "drugi sloj oblike, a dublji slojevi prepoznaju kompleksne objekte.\n",
        "\n",
        "Backpropagation je algoritam koji se koristi za treniranje neuralnih mreža. On izračunava\n",
        "gradijente funkcije gubitka korištenjem lančanog pravila i ažurira težine mreže kako bi\n",
        "minimizirao grešku.\n",
        "\n",
        "Konvolucijske neuralne mreže (CNN) su specijalizirane za obradu slika. Rekurzivne neuralne\n",
        "mreže (RNN) su prikladne za sekvencijalne podatke poput teksta i govora.\n",
        "\"\"\",\n",
        "        \"category\": \"duboko_ucenje\",\n",
        "        \"language\": \"hr\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Veliki jezični modeli (LLM)\",\n",
        "        \"content\": \"\"\"\n",
        "Veliki jezični modeli (Large Language Models - LLM) su neuralne mreže trenirane na ogromnim\n",
        "količinama tekstualnih podataka. Oni mogu generirati tekst, prevoditi jezike, odgovarati na\n",
        "pitanja i obavljati mnoge druge zadatke obrade prirodnog jezika.\n",
        "\n",
        "Transformer arhitektura, predstavljena 2017. godine u radu \"Attention is All You Need\",\n",
        "revolucionirala je obradu prirodnog jezika. Transformeri koriste mehanizam pažnje (attention)\n",
        "koji omogućuje modelu da se fokusira na relevantne dijelove ulaza.\n",
        "\n",
        "GPT (Generative Pre-trained Transformer) modeli su autoregressivni jezični modeli koji\n",
        "predviđaju sljedeći token na temelju prethodnih tokena. BERT (Bidirectional Encoder\n",
        "Representations from Transformers) koristi dvosmjerno kodiranje za bolje razumijevanje konteksta.\n",
        "\n",
        "Prompt engineering je tehnika oblikovanja upita za LLM modele kako bi se dobili kvalitetniji\n",
        "odgovori. Zero-shot i few-shot učenje omogućuju modelima da obavljaju zadatke s malo ili bez primjera.\n",
        "\"\"\",\n",
        "        \"category\": \"llm\",\n",
        "        \"language\": \"hr\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"RAG sustavi\",\n",
        "        \"content\": \"\"\"\n",
        "Retrieval-Augmented Generation (RAG) je tehnika koja kombinira pretraživanje dokumenata s\n",
        "generiranjem teksta pomoću jezičnih modela. RAG sustavi smanjuju halucinacije i omogućuju\n",
        "LLM modelima pristup ažurnim i specifičnim informacijama.\n",
        "\n",
        "RAG se sastoji od dvije glavne faze: indeksiranja i upita. U fazi indeksiranja, dokumenti se\n",
        "dijele na dijelove (chunks), pretvaraju u vektore (embeddings) i pohranjuju u vektorsku bazu podataka.\n",
        "\n",
        "U fazi upita, korisnikov upit se pretvara u vektor i uspoređuje s vektorima dokumenata pomoću\n",
        "kosinusne sličnosti. Najrelevantniji dijelovi dokumenata se dohvaćaju i šalju jezičnom modelu\n",
        "zajedno s upitom.\n",
        "\n",
        "Embedding modeli poput BERT-a ili text-embedding-3 pretvaraju tekst u numeričke vektore koji\n",
        "hvataju semantičko značenje. Vektorske baze podataka poput ChromaDB, Pinecone ili Weaviate\n",
        "omogućuju brzo pretraživanje sličnosti.\n",
        "\n",
        "Strategije chunking-a (dijeljenja dokumenata) i odabir broja dohvaćenih dijelova (top-k)\n",
        "značajno utječu na kvalitetu RAG sustava. Reranking i hibridno pretraživanje dodatno poboljšavaju rezultate.\n",
        "\"\"\",\n",
        "        \"category\": \"rag\",\n",
        "        \"language\": \"hr\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Primjene umjetne inteligencije\",\n",
        "        \"content\": \"\"\"\n",
        "Umjetna inteligencija ima brojne praktične primjene u različitim industrijama. U zdravstvu,\n",
        "AI pomaže u dijagnostici bolesti, analizi medicinskih slika i otkriću novih lijekova.\n",
        "\n",
        "U financijskom sektoru, AI sustavi detektiraju prijevare, automatiziraju trgovanje i procjenjuju\n",
        "kreditnu sposobnost. Chatbotovi i virtualni asistenti koriste NLP za komunikaciju s korisnicima.\n",
        "\n",
        "Autonomna vozila koriste računalni vid, senzore i algoritme strojnog učenja za navigaciju i\n",
        "donošenje odluka u realnom vremenu. Preporuke proizvoda i sadržaja na platformama poput\n",
        "Netflixa i Amazona temelje se na sustavima preporuke.\n",
        "\n",
        "U obrazovanju, AI omogućuje personalizirano učenje prilagođeno potrebama svakog učenika.\n",
        "Sustavi za otkrivanje plagijata i automatsko ocjenjivanje olakšavaju rad nastavnicima.\n",
        "\n",
        "Etička pitanja povezana s AI uključuju pristranost u podacima, privatnost, transparentnost\n",
        "odluka i utjecaj na zapošljavanje. Odgovorna primjena AI zahtijeva pažljivo razmatranje\n",
        "ovih pitanja.\n",
        "\"\"\",\n",
        "        \"category\": \"primjene\",\n",
        "        \"language\": \"hr\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"✓ Created {len(sample_documents)} sample documents\")\n",
        "print(\"\\nDocument titles:\")\n",
        "for i, doc in enumerate(sample_documents, 1):\n",
        "    print(f\"  {i}. {doc['title']} ({doc['category']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "source": [
        "## Part 4: Document Indexing - Building the Vector Database\n",
        "\n",
        "Now we'll process documents and store them in ChromaDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {
        "id": "cell-14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1506cc08-6f88-4b3c-f4d2-478d9013d90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Embedding function created\n"
          ]
        }
      ],
      "source": [
        "class OpenRouterEmbeddingFunction:\n",
        "    \"\"\"\n",
        "    Custom embedding function for ChromaDB using OpenRouter.\n",
        "    \"\"\"\n",
        "    def __init__(self, api_key: str, model: str = EMBEDDING_MODEL):\n",
        "        self.client = OpenAI(\n",
        "            base_url=\"https://openrouter.ai/api/v1\",\n",
        "            api_key=api_key\n",
        "        )\n",
        "        self.model = model\n",
        "\n",
        "    def __call__(self, input: List[str]) -> List[List[float]]:\n",
        "        \"\"\"\n",
        "        Generate embeddings for a list of texts (for documents).\n",
        "        ChromaDB expects this signature for adding documents.\n",
        "        \"\"\"\n",
        "        response = self.client.embeddings.create(\n",
        "            input=input,\n",
        "            model=self.model\n",
        "        )\n",
        "        return [item.embedding for item in response.data]\n",
        "\n",
        "    def embed_query(self, input: str) -> List[List[float]]: # Changed return type hint\n",
        "        \"\"\"\n",
        "        Generate embedding for a single query text.\n",
        "        ChromaDB expects this signature for queries.\n",
        "        \"\"\"\n",
        "        response = self.client.embeddings.create(\n",
        "            input=input,\n",
        "            model=self.model\n",
        "        )\n",
        "        return [response.data[0].embedding] # Wrapped in a list to return List[List[float]]\n",
        "\n",
        "\n",
        "# Create the embedding function instance\n",
        "embedding_function = OpenRouterEmbeddingFunction(OPENROUTER_API_KEY)\n",
        "\n",
        "print(\"✓ Embedding function created\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "872fab47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26712b1b-76f5-4890-b5da-1a0def353ca2"
      },
      "source": [
        "import chromadb\n",
        "\n",
        "# Set to True if you have to re-initialize the collection.\n",
        "# Rerunning the below cell will throw an error if the collection already exists.\n",
        "DELETE_THE_CHROMA_COLLECTION = True\n",
        "\n",
        "if DELETE_THE_CHROMA_COLLECTION:\n",
        "  try:\n",
        "    # Initialize ChromaDB client (if not already done)\n",
        "    client_db = chromadb.Client()\n",
        "\n",
        "    # Delete the collection\n",
        "    client_db.delete_collection(name=\"rag_documents\")\n",
        "\n",
        "    print(\"✓ 'rag_documents' collection deleted.\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error deleting collection: {e}\")\n",
        "\n",
        "# Disable deleting the collection of the next run\n",
        "DELETE_THE_CHROMA_COLLECTION = False"
      ],
      "id": "872fab47",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ 'rag_documents' collection deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {
        "id": "cell-15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac9bdf7-6571-4f1b-d29e-215936df27c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Collection created: 'rag_documents'\n"
          ]
        }
      ],
      "source": [
        "# Initialize ChromaDB client\n",
        "client_db = chromadb.Client()\n",
        "\n",
        "# Create collection\n",
        "collection = client_db.create_collection(\n",
        "    name=\"rag_documents\",\n",
        "    embedding_function=embedding_function,\n",
        "    metadata={\n",
        "        \"description\": \"RAG workshop document collection\",\n",
        "        # Chroma uses L2 distance by default, so we have to explicitly choose cosine distance\n",
        "        \"hnsw:space\": \"cosine\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"✓ Collection created: '{collection.name}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {
        "id": "cell-16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310c859f-d767-4bee-8590-ef6e2c8554ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing and indexing documents...\n",
            "\n",
            "Processing: Uvod u umjetnu inteligenciju\n",
            "  Created 1 chunks\n",
            "Processing: Neuralne mreže i duboko učenje\n",
            "  Created 1 chunks\n",
            "Processing: Veliki jezični modeli (LLM)\n",
            "  Created 1 chunks\n",
            "Processing: RAG sustavi\n",
            "  Created 1 chunks\n",
            "Processing: Primjene umjetne inteligencije\n",
            "  Created 1 chunks\n",
            "\n",
            "Total chunks created: 5\n",
            "\n",
            "Adding to ChromaDB...\n",
            "\n",
            "✓ Indexed 5 chunks in ChromaDB\n",
            "✓ RAG system ready!\n"
          ]
        }
      ],
      "source": [
        "# Process and index documents\n",
        "print(\"Processing and indexing documents...\\n\")\n",
        "\n",
        "all_chunks = []\n",
        "all_ids = []\n",
        "all_metadata = []\n",
        "\n",
        "chunk_id = 0\n",
        "\n",
        "for doc_idx, doc in enumerate(sample_documents):\n",
        "    print(f\"Processing: {doc['title']}\")\n",
        "\n",
        "    # Clean text\n",
        "    cleaned_text = clean_text(doc['content'])\n",
        "\n",
        "    # Chunk the document\n",
        "    chunks = chunk_text_fixed(cleaned_text, chunk_size=200, overlap=30)\n",
        "\n",
        "    print(f\"  Created {len(chunks)} chunks\")\n",
        "\n",
        "    # Add to lists\n",
        "    for chunk_idx, chunk in enumerate(chunks):\n",
        "        all_chunks.append(chunk)\n",
        "        all_ids.append(f\"doc{doc_idx}_chunk{chunk_idx}\")\n",
        "        all_metadata.append({\n",
        "            \"source\": doc['title'],\n",
        "            \"category\": doc['category'],\n",
        "            \"language\": doc['language'],\n",
        "            \"chunk_index\": chunk_idx,\n",
        "            \"total_chunks\": len(chunks)\n",
        "        })\n",
        "        chunk_id += 1\n",
        "\n",
        "print(f\"\\nTotal chunks created: {len(all_chunks)}\")\n",
        "\n",
        "# Add to ChromaDB\n",
        "print(\"\\nAdding to ChromaDB...\")\n",
        "collection.add(\n",
        "    documents=all_chunks,\n",
        "    ids=all_ids,\n",
        "    metadatas=all_metadata\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Indexed {collection.count()} chunks in ChromaDB\")\n",
        "print(f\"✓ RAG system ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_chroma_query(question: str, collection, n_results: int = 5):\n",
        "    \"\"\"\n",
        "    Queries ChromaDB and returns the raw output dictionary for inspection.\n",
        "\n",
        "    Args:\n",
        "        question: The query string.\n",
        "        collection: The ChromaDB collection object.\n",
        "        n_results: The number of results to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        The raw dictionary returned by collection.query().\n",
        "    \"\"\"\n",
        "    print(f\"\\nQuerying ChromaDB for: '{question}'\")\n",
        "    results = collection.query(\n",
        "        query_texts=[question],\n",
        "        n_results=n_results\n",
        "    )\n",
        "    return results\n",
        "\n",
        "# Example usage:\n",
        "sample_question = \"Što su RAG sustavi?\"\n",
        "raw_chroma_output = inspect_chroma_query(sample_question, collection, n_results=3)\n",
        "\n",
        "print(\"\\n--- Raw ChromaDB Output ---\")\n",
        "print(json.dumps(raw_chroma_output, ensure_ascii=False, indent=2))\n",
        "print(\"-------------------------\")\n"
      ],
      "metadata": {
        "id": "wyc8KuwPAA7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3095098-0804-4f20-9969-c8dfe1907eff"
      },
      "id": "wyc8KuwPAA7P",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Querying ChromaDB for: 'Što su RAG sustavi?'\n",
            "\n",
            "--- Raw ChromaDB Output ---\n",
            "{\n",
            "  \"ids\": [\n",
            "    [\n",
            "      \"doc3_chunk0\",\n",
            "      \"doc1_chunk0\",\n",
            "      \"doc4_chunk0\"\n",
            "    ]\n",
            "  ],\n",
            "  \"embeddings\": null,\n",
            "  \"documents\": [\n",
            "    [\n",
            "      \"Retrieval-Augmented Generation (RAG) je tehnika koja kombinira pretraživanje dokumenata s generiranjem teksta pomoću jezičnih modela. RAG sustavi smanjuju halucinacije i omogućuju LLM modelima pristup ažurnim i specifičnim informacijama. RAG se sastoji od dvije glavne faze: indeksiranja i upita. U fazi indeksiranja, dokumenti se dijele na dijelove (chunks), pretvaraju u vektore (embeddings) i pohranjuju u vektorsku bazu podataka. U fazi upita, korisnikov upit se pretvara u vektor i uspoređuje s vektorima dokumenata pomoću kosinusne sličnosti. Najrelevantniji dijelovi dokumenata se dohvaćaju i šalju jezičnom modelu zajedno s upitom. Embedding modeli poput BERT-a ili text-embedding-3 pretvaraju tekst u numeričke vektore koji hvataju semantičko značenje. Vektorske baze podataka poput ChromaDB, Pinecone ili Weaviate omogućuju brzo pretraživanje sličnosti. Strategije chunking-a (dijeljenja dokumenata) i odabir broja dohvaćenih dijelova (top-k) značajno utječu na kvalitetu RAG sustava. Reranking i hibridno pretraživanje dodatno poboljšavaju rezultate.\",\n",
            "      \"Neuralne mreže su računalni modeli inspirirani biološkim neuralnim mrežama. One se sastoje od slojeva međusobno povezanih čvorova (neurona) koji obrađuju informacije. Duboko učenje koristi neuralne mreže s više skrivenih slojeva. Svaki sloj uči sve kompleksnije značajke iz podataka. Na primjer, u prepoznavanju slika, prvi sloj može naučiti rubove, drugi sloj oblike, a dublji slojevi prepoznaju kompleksne objekte. Backpropagation je algoritam koji se koristi za treniranje neuralnih mreža. On izračunava gradijente funkcije gubitka korištenjem lančanog pravila i ažurira težine mreže kako bi minimizirao grešku. Konvolucijske neuralne mreže (CNN) su specijalizirane za obradu slika. Rekurzivne neuralne mreže (RNN) su prikladne za sekvencijalne podatke poput teksta i govora.\",\n",
            "      \"Umjetna inteligencija ima brojne praktične primjene u različitim industrijama. U zdravstvu, AI pomaže u dijagnostici bolesti, analizi medicinskih slika i otkriću novih lijekova. U financijskom sektoru, AI sustavi detektiraju prijevare, automatiziraju trgovanje i procjenjuju kreditnu sposobnost. Chatbotovi i virtualni asistenti koriste NLP za komunikaciju s korisnicima. Autonomna vozila koriste računalni vid, senzore i algoritme strojnog učenja za navigaciju i donošenje odluka u realnom vremenu. Preporuke proizvoda i sadržaja na platformama poput Netflixa i Amazona temelje se na sustavima preporuke. U obrazovanju, AI omogućuje personalizirano učenje prilagođeno potrebama svakog učenika. Sustavi za otkrivanje plagijata i automatsko ocjenjivanje olakšavaju rad nastavnicima. Etička pitanja povezana s AI uključuju pristranost u podacima, privatnost, transparentnost odluka i utjecaj na zapošljavanje. Odgovorna primjena AI zahtijeva pažljivo razmatranje ovih pitanja.\"\n",
            "    ]\n",
            "  ],\n",
            "  \"uris\": null,\n",
            "  \"included\": [\n",
            "    \"metadatas\",\n",
            "    \"documents\",\n",
            "    \"distances\"\n",
            "  ],\n",
            "  \"data\": null,\n",
            "  \"metadatas\": [\n",
            "    [\n",
            "      {\n",
            "        \"category\": \"rag\",\n",
            "        \"source\": \"RAG sustavi\",\n",
            "        \"language\": \"hr\",\n",
            "        \"chunk_index\": 0,\n",
            "        \"total_chunks\": 1\n",
            "      },\n",
            "      {\n",
            "        \"source\": \"Neuralne mreže i duboko učenje\",\n",
            "        \"chunk_index\": 0,\n",
            "        \"category\": \"duboko_ucenje\",\n",
            "        \"language\": \"hr\",\n",
            "        \"total_chunks\": 1\n",
            "      },\n",
            "      {\n",
            "        \"language\": \"hr\",\n",
            "        \"total_chunks\": 1,\n",
            "        \"chunk_index\": 0,\n",
            "        \"source\": \"Primjene umjetne inteligencije\",\n",
            "        \"category\": \"primjene\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"distances\": [\n",
            "    [\n",
            "      0.4205925464630127,\n",
            "      0.6659459471702576,\n",
            "      0.6801036596298218\n",
            "    ]\n",
            "  ]\n",
            "}\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {
        "id": "cell-17"
      },
      "source": [
        "## Part 5: The RAG Query Pipeline\n",
        "\n",
        "Now we'll implement the complete query pipeline that:\n",
        "1. Takes a user question\n",
        "2. Retrieves relevant chunks from ChromaDB\n",
        "3. Constructs a prompt with context\n",
        "4. Generates an answer using an LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18",
      "metadata": {
        "id": "cell-18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6178d718-218d-46fd-ce4c-14ce3c43b113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ RAG query function defined\n"
          ]
        }
      ],
      "source": [
        "def query_rag(question: str, collection, n_results: int = 5, model: str = LLM_MODEL,\n",
        "              show_sources: bool = True) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    Complete RAG query pipeline.\n",
        "\n",
        "    Args:\n",
        "        question: User's question\n",
        "        collection: ChromaDB collection\n",
        "        n_results: Number of chunks to retrieve\n",
        "        model: LLM model to use for generation\n",
        "        show_sources: Whether to display retrieved sources\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with answer and metadata\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve relevant chunks\n",
        "    results = collection.query(\n",
        "        query_texts=[question],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    retrieved_chunks = results['documents'][0]\n",
        "    metadatas = results['metadatas'][0]\n",
        "    distances = results['distances'][0]\n",
        "\n",
        "    if show_sources:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RETRIEVED SOURCES\")\n",
        "        print(\"=\"*80)\n",
        "        for i, (chunk, meta, dist) in enumerate(zip(retrieved_chunks, metadatas, distances), 1):\n",
        "            # Chroma returns cosine distance instead of cosine similarity, so we have to manually convert it back\n",
        "            similarity = 1 - dist\n",
        "            print(f\"\\n[{i}] Similarity: {similarity:.3f} | Source: {meta['source']} | Category: {meta['category']}\")\n",
        "            print(f\"    {chunk[:200]}...\")\n",
        "\n",
        "    # Step 2: Construct prompt with context\n",
        "    #chunk_list = []\n",
        "    #for i, chunk in enumerate(retrieved_chunks):\n",
        "    #    chunk_list.append(f\"Dokument {i+1}: {chunk}\")\n",
        "    #context = \"\\n\\n\".join(chunk_list)\n",
        "\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"[Dokument {i+1}]: {chunk}\"\n",
        "        for i, chunk in enumerate(retrieved_chunks)\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"Ti si pametan asistent koji odgovara na pitanja na temelju dostavljenog konteksta.\n",
        "\n",
        "Kontekst:\n",
        "{context}\n",
        "\n",
        "Pitanje: {question}\n",
        "\n",
        "Upute:\n",
        "- Koristi SAMO informacije iz dostavljenog konteksta\n",
        "- Ako odgovor nije u kontekstu, reci \"Nemam tu informaciju u dostavljenim dokumentima\"\n",
        "- Citiraj koji dokument si koristio za svaku tvrdnju (npr. \"Prema Dokumentu 1...\")\n",
        "- Budi koncizan ali potpun u odgovoru\n",
        "- Odgovaraj na hrvatskom jeziku\n",
        "\n",
        "Odgovor:\"\"\"\n",
        "\n",
        "    # Step 3: Generate answer with LLM\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.3,  # Lower temperature for more factual responses\n",
        "        max_tokens=1000\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"sources\": metadatas,\n",
        "        \"retrieved_chunks\": retrieved_chunks,\n",
        "        \"similarities\": [1 - d for d in distances]\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"✓ RAG query function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {
        "id": "cell-19"
      },
      "source": [
        "## Part 6: Testing the RAG System\n",
        "\n",
        "Let's test our RAG system with various questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20",
      "metadata": {
        "id": "cell-20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1dd523-f1d1-44b1-c562-4da97e3e3801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RETRIEVED SOURCES\n",
            "================================================================================\n",
            "\n",
            "[1] Similarity: 0.631 | Source: Neuralne mreže i duboko učenje | Category: duboko_ucenje\n",
            "    Neuralne mreže su računalni modeli inspirirani biološkim neuralnim mrežama. One se sastoje od slojeva međusobno povezanih čvorova (neurona) koji obrađuju informacije. Duboko učenje koristi neuralne mr...\n",
            "\n",
            "[2] Similarity: 0.371 | Source: RAG sustavi | Category: rag\n",
            "    Retrieval-Augmented Generation (RAG) je tehnika koja kombinira pretraživanje dokumenata s generiranjem teksta pomoću jezičnih modela. RAG sustavi smanjuju halucinacije i omogućuju LLM modelima pristup...\n",
            "\n",
            "[3] Similarity: 0.364 | Source: Veliki jezični modeli (LLM) | Category: llm\n",
            "    Veliki jezični modeli (Large Language Models - LLM) su neuralne mreže trenirane na ogromnim količinama tekstualnih podataka. Oni mogu generirati tekst, prevoditi jezike, odgovarati na pitanja i obavlj...\n",
            "\n",
            "================================================================================\n",
            "PITANJE: Što je backpropagation i kako funkcionira?\n",
            "================================================================================\n",
            "\n",
            "ODGOVOR:\n",
            "Backpropagation je algoritam koji se koristi za treniranje neuralnih mreža. On izračunava gradijente funkcije gubitka korištenjem lančanog pravila i ažurira težine mreže kako bi minimizirao grešku. Prema Dokumentu 1, ovaj proces omogućuje mreži da uči iz podataka i poboljšava svoje performanse tijekom treniranja.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test Query 1: Technical definition\n",
        "question1 = \"Što je backpropagation i kako funkcionira?\"\n",
        "\n",
        "result1 = query_rag(question1, collection, n_results=3)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PITANJE:\", question1)\n",
        "print(\"=\"*80)\n",
        "print(\"\\nODGOVOR:\")\n",
        "print(result1[\"answer\"])\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-21",
      "metadata": {
        "id": "cell-21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0edc72-7a7c-4de2-da9d-3fef4b07b2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RETRIEVED SOURCES\n",
            "================================================================================\n",
            "\n",
            "[1] Similarity: 0.442 | Source: Veliki jezični modeli (LLM) | Category: llm\n",
            "    Veliki jezični modeli (Large Language Models - LLM) su neuralne mreže trenirane na ogromnim količinama tekstualnih podataka. Oni mogu generirati tekst, prevoditi jezike, odgovarati na pitanja i obavlj...\n",
            "\n",
            "[2] Similarity: 0.296 | Source: Neuralne mreže i duboko učenje | Category: duboko_ucenje\n",
            "    Neuralne mreže su računalni modeli inspirirani biološkim neuralnim mrežama. One se sastoje od slojeva međusobno povezanih čvorova (neurona) koji obrađuju informacije. Duboko učenje koristi neuralne mr...\n",
            "\n",
            "[3] Similarity: 0.289 | Source: RAG sustavi | Category: rag\n",
            "    Retrieval-Augmented Generation (RAG) je tehnika koja kombinira pretraživanje dokumenata s generiranjem teksta pomoću jezičnih modela. RAG sustavi smanjuju halucinacije i omogućuju LLM modelima pristup...\n",
            "\n",
            "[4] Similarity: 0.286 | Source: Uvod u umjetnu inteligenciju | Category: osnove\n",
            "    Umjetna inteligencija (UI) je područje računarstva koje se bavi stvaranjem inteligentnih sustava koji mogu učiti, rasuđivati i rješavati probleme. UI tehnologije uključuju strojno učenje, duboko učenj...\n",
            "\n",
            "[5] Similarity: 0.251 | Source: Primjene umjetne inteligencije | Category: primjene\n",
            "    Umjetna inteligencija ima brojne praktične primjene u različitim industrijama. U zdravstvu, AI pomaže u dijagnostici bolesti, analizi medicinskih slika i otkriću novih lijekova. U financijskom sektoru...\n",
            "\n",
            "================================================================================\n",
            "PITANJE: Kada je predstavljena transformer arhitektura?\n",
            "================================================================================\n",
            "\n",
            "ODGOVOR:\n",
            "Transformer arhitektura je predstavljena 2017. godine u radu \"Attention is All You Need\". Prema Dokumentu 1, ova arhitektura je revolucionirala obradu prirodnog jezika.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test Query 2: Factual information\n",
        "question2 = \"Kada je predstavljena transformer arhitektura?\"\n",
        "\n",
        "result2 = query_rag(question2, collection, n_results=5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PITANJE:\", question2)\n",
        "print(\"=\"*80)\n",
        "print(\"\\nODGOVOR:\")\n",
        "print(result2[\"answer\"])\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-22",
      "metadata": {
        "id": "cell-22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71eccb52-b0a9-46f3-8dc8-61267a40f51c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RETRIEVED SOURCES\n",
            "================================================================================\n",
            "\n",
            "[1] Similarity: 0.488 | Source: RAG sustavi | Category: rag\n",
            "    Retrieval-Augmented Generation (RAG) je tehnika koja kombinira pretraživanje dokumenata s generiranjem teksta pomoću jezičnih modela. RAG sustavi smanjuju halucinacije i omogućuju LLM modelima pristup...\n",
            "\n",
            "[2] Similarity: 0.315 | Source: Primjene umjetne inteligencije | Category: primjene\n",
            "    Umjetna inteligencija ima brojne praktične primjene u različitim industrijama. U zdravstvu, AI pomaže u dijagnostici bolesti, analizi medicinskih slika i otkriću novih lijekova. U financijskom sektoru...\n",
            "\n",
            "[3] Similarity: 0.302 | Source: Neuralne mreže i duboko učenje | Category: duboko_ucenje\n",
            "    Neuralne mreže su računalni modeli inspirirani biološkim neuralnim mrežama. One se sastoje od slojeva međusobno povezanih čvorova (neurona) koji obrađuju informacije. Duboko učenje koristi neuralne mr...\n",
            "\n",
            "[4] Similarity: 0.275 | Source: Uvod u umjetnu inteligenciju | Category: osnove\n",
            "    Umjetna inteligencija (UI) je područje računarstva koje se bavi stvaranjem inteligentnih sustava koji mogu učiti, rasuđivati i rješavati probleme. UI tehnologije uključuju strojno učenje, duboko učenj...\n",
            "\n",
            "[5] Similarity: 0.263 | Source: Veliki jezični modeli (LLM) | Category: llm\n",
            "    Veliki jezični modeli (Large Language Models - LLM) su neuralne mreže trenirane na ogromnim količinama tekstualnih podataka. Oni mogu generirati tekst, prevoditi jezike, odgovarati na pitanja i obavlj...\n",
            "\n",
            "================================================================================\n",
            "PITANJE: Objasni kako RAG sustavi smanjuju halucinacije?\n",
            "================================================================================\n",
            "\n",
            "ODGOVOR:\n",
            "RAG sustavi smanjuju halucinacije tako što kombiniraju pretraživanje dokumenata s generiranjem teksta pomoću jezičnih modela, što omogućuje LLM modelima pristup ažurnim i specifičnim informacijama. Ova tehnika uključuje faze indeksiranja i upita, gdje se najrelevantniji dijelovi dokumenata dohvaćaju i šalju jezičnom modelu zajedno s upitom. Time se osigurava da generirani sadržaj bude temeljen na stvarnim podacima, čime se smanjuje rizik od halucinacija, odnosno generiranja netočnih informacija. Prema Dokumentu 1, RAG sustavi smanjuju halucinacije i omogućuju LLM modelima pristup ažurnim i specifičnim informacijama.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test Query 3: Conceptual explanation\n",
        "question3 = \"Objasni kako RAG sustavi smanjuju halucinacije?\"\n",
        "\n",
        "result3 = query_rag(question3, collection, n_results=5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PITANJE:\", question3)\n",
        "print(\"=\"*80)\n",
        "print(\"\\nODGOVOR:\")\n",
        "print(result3[\"answer\"])\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-23",
      "metadata": {
        "id": "cell-23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffca8326-ca13-4143-96c0-d5e238fddfee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RETRIEVED SOURCES\n",
            "================================================================================\n",
            "\n",
            "[1] Similarity: 0.384 | Source: Veliki jezični modeli (LLM) | Category: llm\n",
            "    Veliki jezični modeli (Large Language Models - LLM) su neuralne mreže trenirane na ogromnim količinama tekstualnih podataka. Oni mogu generirati tekst, prevoditi jezike, odgovarati na pitanja i obavlj...\n",
            "\n",
            "[2] Similarity: 0.300 | Source: RAG sustavi | Category: rag\n",
            "    Retrieval-Augmented Generation (RAG) je tehnika koja kombinira pretraživanje dokumenata s generiranjem teksta pomoću jezičnih modela. RAG sustavi smanjuju halucinacije i omogućuju LLM modelima pristup...\n",
            "\n",
            "[3] Similarity: 0.240 | Source: Primjene umjetne inteligencije | Category: primjene\n",
            "    Umjetna inteligencija ima brojne praktične primjene u različitim industrijama. U zdravstvu, AI pomaže u dijagnostici bolesti, analizi medicinskih slika i otkriću novih lijekova. U financijskom sektoru...\n",
            "\n",
            "[4] Similarity: 0.227 | Source: Uvod u umjetnu inteligenciju | Category: osnove\n",
            "    Umjetna inteligencija (UI) je područje računarstva koje se bavi stvaranjem inteligentnih sustava koji mogu učiti, rasuđivati i rješavati probleme. UI tehnologije uključuju strojno učenje, duboko učenj...\n",
            "\n",
            "[5] Similarity: 0.224 | Source: Neuralne mreže i duboko učenje | Category: duboko_ucenje\n",
            "    Neuralne mreže su računalni modeli inspirirani biološkim neuralnim mrežama. One se sastoje od slojeva međusobno povezanih čvorova (neurona) koji obrađuju informacije. Duboko učenje koristi neuralne mr...\n",
            "\n",
            "================================================================================\n",
            "PITANJE: Koliko parametara ima GPT-5?\n",
            "================================================================================\n",
            "\n",
            "ODGOVOR:\n",
            "Nemam tu informaciju u dostavljenim dokumentima. Dokumenti ne navode broj parametara za GPT-5.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test Query 4: Information not in documents\n",
        "question4 = \"Koliko parametara ima GPT-5?\"\n",
        "\n",
        "result4 = query_rag(question4, collection, n_results=5,model='mistralai/mistral-large-2512')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PITANJE:\", question4)\n",
        "print(\"=\"*80)\n",
        "print(\"\\nODGOVOR:\")\n",
        "print(result4[\"answer\"])\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-24",
      "metadata": {
        "id": "cell-24"
      },
      "source": [
        "## Part 7: RAG vs Non-RAG Comparison\n",
        "\n",
        "Let's compare answers with and without RAG to see the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-25",
      "metadata": {
        "id": "cell-25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0157813-d34f-4de4-efdb-3e72f269132e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Non-RAG query function defined\n"
          ]
        }
      ],
      "source": [
        "def query_without_rag(question: str, model: str = LLM_MODEL) -> str:\n",
        "    \"\"\"\n",
        "    Query LLM directly without RAG context.\n",
        "\n",
        "    Args:\n",
        "        question: User's question\n",
        "        model: LLM model to use\n",
        "\n",
        "    Returns:\n",
        "        Generated answer\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Ti si pametan asistent koji odgovara na pitanja.\n",
        "\n",
        "Pitanje: {question}\n",
        "\n",
        "Upute:\n",
        "- Ako ne znaš odgovor ili nemaš dovoljno informacija, reci da ne znaš.\n",
        "- Budi koncizan ali potpun u odgovoru.\n",
        "- Odgovaraj na hrvatskom jeziku.\n",
        "\n",
        "Odgovor:\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "print(\"✓ Non-RAG query function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-26",
      "metadata": {
        "id": "cell-26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4bd93b-4fb1-44a0-c34e-d71201de4e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PITANJE: Koliko parametara ima GPT-5 model\n",
            "MODEL:   qwen/qwen3-vl-30b-a3b-thinking\n",
            "================================================================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "SA RAG-om (s kontekstom iz dokumenata):\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Nemam tu informaciju u dostavljenim dokumentima. \n",
            "\n",
            "Ni u jednom od tri dokumenta (Dokument 1, Dokument 2 ili Dokument 3) se ne spominje broj parametara za GPT-5 model. Dokumenti opisuju opće koncepte velikih jezičnih modela, Transformer arhitekture i RAG tehnologiju, ali ne sadrže specifične informacije o broju parametara za GPT-5.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "BEZ RAG-a (samo znanje modela):\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "GPT-5 model još nije objavljen, pa nema dostupnih informacija o broju parametara. Trenutno najnoviji model je GPT-4.\n",
            "\n",
            "================================================================================\n",
            "ANALIZA:\n",
            "================================================================================\n",
            "\n",
            "RAG prednosti:\n",
            "✓ Temelji odgovor na specifičnim dokumentima\n",
            "✓ Može citirati izvore\n",
            "✓ Smanjuje halucinacije\n",
            "✓ Omogućuje ažuriranje znanja bez ponovnog treniranja modela\n",
            "\n",
            "Bez RAG-a:\n",
            "✗ Ovisi samo o znanju iz treninga\n",
            "✗ Može generirati netočne informacije\n",
            "✗ Nema izvora za provjeru\n",
            "✓ Ali može biti brži za opća pitanja\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compare on a specific question\n",
        "comparison_question = \"Koliko parametara ima GPT-5 model\"\n",
        "TEST_MODEL  = 'qwen/qwen3-vl-30b-a3b-thinking'\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PITANJE:\", comparison_question)\n",
        "print(\"MODEL:  \", TEST_MODEL)\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "# Get RAG answer\n",
        "rag_answer = query_rag(comparison_question, collection, n_results=3, show_sources=False, model=TEST_MODEL)\n",
        "\n",
        "# Get non-RAG answer\n",
        "no_rag_answer = query_without_rag(comparison_question, model=TEST_MODEL)\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SA RAG-om (s kontekstom iz dokumenata):\")\n",
        "print(\"-\"*80)\n",
        "print(rag_answer[\"answer\"])\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"BEZ RAG-a (samo znanje modela):\")\n",
        "print(\"-\"*80)\n",
        "print(no_rag_answer)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALIZA:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "RAG prednosti:\n",
        "✓ Temelji odgovor na specifičnim dokumentima\n",
        "✓ Može citirati izvore\n",
        "✓ Smanjuje halucinacije\n",
        "✓ Omogućuje ažuriranje znanja bez ponovnog treniranja modela\n",
        "\n",
        "Bez RAG-a:\n",
        "✗ Ovisi samo o znanju iz treninga\n",
        "✗ Može generirati netočne informacije\n",
        "✗ Nema izvora za provjeru\n",
        "✓ Ali može biti brži za opća pitanja\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-27",
      "metadata": {
        "id": "cell-27"
      },
      "source": [
        "## Part 8: Experiments - Optimizing RAG Configuration\n",
        "\n",
        "Let's experiment with different RAG configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-28",
      "metadata": {
        "id": "cell-28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b88b966e-c7ff-4162-cacf-a43ab6124429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EKSPERIMENT 1: Usporedba različitih top-k vrijednosti\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Testiranje s top-3 rezultata\n",
            "================================================================================\n",
            "\n",
            "Odgovor:\n",
            "RAG (Retrieval-Augmented Generation) je tehnika koja kombinira pretraživanje dokumenata s generiranjem teksta pomoću jezičnih modela. Funkcionira u dvije glavne faze: indeksiranja i upita. U fazi indeksiranja, dokumenti se dijele na dijelove (chunks), pretvaraju u vektore (embeddings) i pohranjuju u vektorsku bazu podataka. U fazi upita, korisnikov upit se pretvara u vektor i uspoređuje s vektorima dokumenata pomoću kosinusne sličnosti. Najrelevantniji dijelovi dokumenata se dohvaćaju i šalju jezičnom modelu zajedno s upitom. Embedding modeli poput BERT-a ili text-embedding-3 koriste se za pretvaranje teksta u numeričke vektore koji hvataju semantičko značenje. Vektorske baze podataka poput ChromaDB, Pinecone ili Weaviate omogućuju brzo pretraživanje sličnosti. Kvalitetu RAG sustava značajno utječu strategije chunking-a i odabir broja dohvaćenih dijelova (top-k) (prema Dokumentu 1).\n",
            "\n",
            "Broj dohvaćenih izvora: 3\n",
            "Prosječna sličnost: 0.421\n",
            "\n",
            "================================================================================\n",
            "Testiranje s top-5 rezultata\n",
            "================================================================================\n",
            "\n",
            "Odgovor:\n",
            "RAG (Retrieval-Augmented Generation) je tehnika koja kombinira pretraživanje dokumenata s generiranjem teksta pomoću jezičnih modela. Funkcionira u dvije glavne faze: indeksiranja i upita. \n",
            "\n",
            "U fazi indeksiranja, dokumenti se dijele na dijelove (chunks), pretvaraju u vektore (embeddings) i pohranjuju u vektorsku bazu podataka. U fazi upita, korisnikov upit se pretvara u vektor i uspoređuje s vektorima dokumenata pomoću kosinusne sličnosti. Najrelevantniji dijelovi dokumenata se dohvaćaju i šalju jezičnom modelu zajedno s upitom. Ova tehnika smanjuje halucinacije i omogućuje LLM modelima pristup ažurnim i specifičnim informacijama (prema Dokumentu 1).\n",
            "\n",
            "Broj dohvaćenih izvora: 5\n",
            "Prosječna sličnost: 0.368\n",
            "\n",
            "================================================================================\n",
            "Testiranje s top-10 rezultata\n",
            "================================================================================\n",
            "\n",
            "Odgovor:\n",
            "RAG (Retrieval-Augmented Generation) je tehnika koja kombinira pretraživanje dokumenata s generiranjem teksta pomoću jezičnih modela. Funkcionira u dvije glavne faze: indeksiranja i upita. U fazi indeksiranja, dokumenti se dijele na dijelove (chunks), pretvaraju u vektore (embeddings) i pohranjuju u vektorsku bazu podataka. U fazi upita, korisnikov upit se pretvara u vektor i uspoređuje s vektorima dokumenata pomoću kosinusne sličnosti. Najrelevantniji dijelovi dokumenata se dohvaćaju i šalju jezičnom modelu zajedno s upitom. Ova tehnika smanjuje halucinacije i omogućuje LLM modelima pristup ažurnim i specifičnim informacijama (Prema Dokumentu 1).\n",
            "\n",
            "Broj dohvaćenih izvora: 5\n",
            "Prosječna sličnost: 0.368\n"
          ]
        }
      ],
      "source": [
        "# Experiment 1: Varying number of retrieved chunks (top-k)\n",
        "\n",
        "test_question = \"Što je RAG i kako funkcionira?\"\n",
        "\n",
        "print(\"EKSPERIMENT 1: Usporedba različitih top-k vrijednosti\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "top_k_values = [3, 5, 10]\n",
        "\n",
        "for k in top_k_values:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testiranje s top-{k} rezultata\")\n",
        "    print('='*80)\n",
        "\n",
        "    result = query_rag(test_question, collection, n_results=k, show_sources=False)\n",
        "\n",
        "    print(f\"\\nOdgovor:\")\n",
        "    print(result[\"answer\"])\n",
        "\n",
        "    print(f\"\\nBroj dohvaćenih izvora: {len(result['sources'])}\")\n",
        "    print(f\"Prosječna sličnost: {sum(result['similarities']) / len(result['similarities']):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-29",
      "metadata": {
        "id": "cell-29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f1ef8c-cdab-426c-b71c-b19787cddc73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EKSPERIMENT 2: Pretraživanje s filterima\n",
            "================================================================================\n",
            "\n",
            "Pitanje: Objasni duboko učenje\n",
            "Filter: category='duboko_ucenje'\n",
            "\n",
            "Dohvaćeni dokumenti:\n",
            "\n",
            "[1] Izvor: Neuralne mreže i duboko učenje | Kategorija: duboko_ucenje | Sličnost: 0.437\n",
            "    Neuralne mreže su računalni modeli inspirirani biološkim neuralnim mrežama. One se sastoje od slojeva međusobno povezanih čvorova (neurona) koji obrađ...\n"
          ]
        }
      ],
      "source": [
        "# Experiment 2: Metadata filtering\n",
        "\n",
        "print(\"\\nEKSPERIMENT 2: Pretraživanje s filterima\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "question = \"Objasni duboko učenje\"\n",
        "\n",
        "# Query with category filter\n",
        "filtered_results = collection.query(\n",
        "    query_texts=[question],\n",
        "    n_results=5,\n",
        "    where={\"category\": \"duboko_ucenje\"}  # Filter by category\n",
        ")\n",
        "\n",
        "print(f\"\\nPitanje: {question}\")\n",
        "print(f\"Filter: category='duboko_ucenje'\")\n",
        "print(\"\\nDohvaćeni dokumenti:\")\n",
        "\n",
        "for i, (doc, meta, dist) in enumerate(\n",
        "    zip(\n",
        "        filtered_results['documents'][0],\n",
        "        filtered_results['metadatas'][0],\n",
        "        filtered_results['distances'][0]\n",
        "    ), 1\n",
        "):\n",
        "    print(f\"\\n[{i}] Izvor: {meta['source']} | Kategorija: {meta['category']} | Sličnost: {1-dist:.3f}\")\n",
        "    print(f\"    {doc[:150]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-30",
      "metadata": {
        "id": "cell-30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e6a191-84f5-4e49-a312-447c1c35a65e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EKSPERIMENT 3: Odgovori s pragom pouzdanosti\n",
            "================================================================================\n",
            "\n",
            "Pitanje: Što je transformer arhitektura?\n",
            "Pouzdanost: 0.400\n",
            "Odgovoreno: False\n",
            "\n",
            "Odgovor:\n",
            "⚠️ Pouzdanost odgovora je preniska (max sličnost: 0.400). Nemam dovoljno relevantnih informacija za odgovor na ovo pitanje....\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Pitanje: Koja je najbolja pizza u Zagrebu?\n",
            "Pouzdanost: 0.198\n",
            "Odgovoreno: False\n",
            "\n",
            "Odgovor:\n",
            "⚠️ Pouzdanost odgovora je preniska (max sličnost: 0.198). Nemam dovoljno relevantnih informacija za odgovor na ovo pitanje.\n"
          ]
        }
      ],
      "source": [
        "# Experiment 3: Answer quality with confidence thresholds\n",
        "\n",
        "def query_rag_with_confidence(question: str, collection, threshold: float = 0.7,\n",
        "                               n_results: int = 5) -> Dict[str, any]:\n",
        "    \"\"\"\n",
        "    RAG query with confidence threshold - only answer if relevance is high enough.\n",
        "\n",
        "    Args:\n",
        "        question: User's question\n",
        "        collection: ChromaDB collection\n",
        "        threshold: Minimum similarity threshold (0-1)\n",
        "        n_results: Number of chunks to retrieve\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with answer and confidence info\n",
        "    \"\"\"\n",
        "    results = collection.query(\n",
        "        query_texts=[question],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    distances = results['distances'][0]\n",
        "    similarities = [1 - d for d in distances]\n",
        "    max_similarity = max(similarities)\n",
        "\n",
        "    if max_similarity < threshold:\n",
        "        return {\n",
        "            \"answer\": f\"⚠️ Pouzdanost odgovora je preniska (max sličnost: {max_similarity:.3f}). Nemam dovoljno relevantnih informacija za odgovor na ovo pitanje.\",\n",
        "            \"confidence\": max_similarity,\n",
        "            \"answered\": False\n",
        "        }\n",
        "\n",
        "    # Proceed with normal RAG if confidence is sufficient\n",
        "    result = query_rag(question, collection, n_results=n_results, show_sources=False)\n",
        "    result[\"confidence\"] = max_similarity\n",
        "    result[\"answered\"] = True\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"EKSPERIMENT 3: Odgovori s pragom pouzdanosti\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test with relevant question\n",
        "relevant_q = \"Što je transformer arhitektura?\"\n",
        "result_relevant = query_rag_with_confidence(relevant_q, collection, threshold=0.6)\n",
        "\n",
        "print(f\"\\nPitanje: {relevant_q}\")\n",
        "print(f\"Pouzdanost: {result_relevant['confidence']:.3f}\")\n",
        "print(f\"Odgovoreno: {result_relevant['answered']}\")\n",
        "print(f\"\\nOdgovor:\\n{result_relevant['answer'][:300]}...\")\n",
        "\n",
        "# Test with irrelevant question\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "irrelevant_q = \"Koja je najbolja pizza u Zagrebu?\"\n",
        "result_irrelevant = query_rag_with_confidence(irrelevant_q, collection, threshold=0.6)\n",
        "\n",
        "print(f\"\\nPitanje: {irrelevant_q}\")\n",
        "print(f\"Pouzdanost: {result_irrelevant['confidence']:.3f}\")\n",
        "print(f\"Odgovoreno: {result_irrelevant['answered']}\")\n",
        "print(f\"\\nOdgovor:\\n{result_irrelevant['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-31",
      "metadata": {
        "id": "cell-31"
      },
      "source": [
        "## Part 9: Interactive RAG Demo\n",
        "\n",
        "Try your own questions!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Gs8QuogFQEo"
      },
      "id": "5Gs8QuogFQEo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "980b40f1"
      },
      "source": [
        "### Analiza Eksperimenta 3: Odgovori s Pragom Pouzdanosti\n",
        "\n",
        "U Eksperimentu 3 testirali smo funkcionalnost `query_rag_with_confidence` koja koristi prag sličnosti za procjenu pouzdanosti odgovora. Prag je postavljen na `0.6`.\n",
        "\n",
        "- **Relevantno pitanje (\"Što je transformer arhitektura?\")**: Maksimalna sličnost dohvaćenih dokumenata bila je `0.400`. Budući da je ova vrijednost niža od praga od `0.6`, sustav je ispravno prepoznao da nema dovoljno relevantnih informacija i vratio poruku o niskoj pouzdanosti, umjesto da pokuša generirati odgovor koji bi mogao biti netočan ili haluciniran.\n",
        "\n",
        "- **Irelevantno pitanje (\"Koja je najbolja pizza u Zagrebu?\")**: Očekivano, za ovo pitanje je maksimalna sličnost bila vrlo niska (`0.198`), daleko ispod praga, što je rezultiralo istom porukom o niskoj pouzdanosti. Ovo demonstrira kako prag pomaže sustavu da prepozna kada pitanje nije pokriveno njegovim znanjem.\n",
        "\n",
        "**Zaključak:** Postavljanje praga pouzdanosti ključno je za poboljšanje robusnosti RAG sustava, jer sprječava davanje odgovora kada je relevantnost dohvaćenih informacija upitna. Međutim, važno je kalibrirati prag specifično za korišteni embedding model i domenu, budući da različiti modeli mogu proizvoditi različite distribucije sličnosti."
      ],
      "id": "980b40f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-32",
      "metadata": {
        "id": "cell-32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5fd042c-aff5-4ac5-9841-f99fb3bc9532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RETRIEVED SOURCES\n",
            "================================================================================\n",
            "\n",
            "[1] Similarity: 0.708 | Source: Primjene umjetne inteligencije | Category: primjene\n",
            "    Umjetna inteligencija ima brojne praktične primjene u različitim industrijama. U zdravstvu, AI pomaže u dijagnostici bolesti, analizi medicinskih slika i otkriću novih lijekova. U financijskom sektoru...\n",
            "\n",
            "[2] Similarity: 0.613 | Source: Uvod u umjetnu inteligenciju | Category: osnove\n",
            "    Umjetna inteligencija (UI) je područje računarstva koje se bavi stvaranjem inteligentnih sustava koji mogu učiti, rasuđivati i rješavati probleme. UI tehnologije uključuju strojno učenje, duboko učenj...\n",
            "\n",
            "================================================================================\n",
            "PITANJE: Koje su primjene umjetne inteligencije u zdravstvu?\n",
            "================================================================================\n",
            "\n",
            "ODGOVOR:\n",
            "Prema Dokumentu 1, umjetna inteligencija u zdravstvu pomaže u dijagnostici bolesti, analizi medicinskih slika i otkriću novih lijekova.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Interactive query - change this to any question you want\n",
        "your_question = \"Koje su primjene umjetne inteligencije u zdravstvu?\"\n",
        "\n",
        "result = query_rag(your_question, collection, n_results=2, model='deepseek/deepseek-v3.2-speciale')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PITANJE:\", your_question)\n",
        "print(\"=\"*80)\n",
        "print(\"\\nODGOVOR:\")\n",
        "print(result[\"answer\"])\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-33",
      "metadata": {
        "id": "cell-33"
      },
      "source": [
        "## Summary and Key Takeaways\n",
        "\n",
        "### What We Built\n",
        "\n",
        "1. **Complete RAG Pipeline**\n",
        "   - Document loading and processing\n",
        "   - Smart chunking strategies\n",
        "   - Vector storage with ChromaDB\n",
        "   - Retrieval + Generation workflow\n",
        "\n",
        "2. **Key Components**\n",
        "   - **Indexing**: Documents → Chunks → Embeddings → Vector DB\n",
        "   - **Retrieval**: Query → Embedding → Similarity Search → Top-K chunks\n",
        "   - **Generation**: Context + Query → LLM → Answer\n",
        "\n",
        "3. **Configuration Parameters**\n",
        "   - Chunk size and overlap\n",
        "   - Number of retrieved chunks (top-k)\n",
        "   - Similarity threshold\n",
        "   - LLM temperature\n",
        "\n",
        "### RAG Benefits\n",
        "\n",
        "✅ **Reduces hallucinations** - Grounds answers in documents  \n",
        "✅ **Source attribution** - Can cite specific documents  \n",
        "✅ **Up-to-date information** - Add new docs without retraining  \n",
        "✅ **Domain-specific** - Works with specialized knowledge  \n",
        "✅ **Controllable** - Filter by metadata, adjust parameters  \n",
        "\n",
        "### When to Use RAG\n",
        "\n",
        "- ✓ Domain-specific Q&A systems\n",
        "- ✓ Document search and analysis\n",
        "- ✓ Customer support with knowledge bases\n",
        "- ✓ Research assistants\n",
        "- ✓ Compliance and legal document queries\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Chunking**: 256-1024 tokens, 10-20% overlap\n",
        "2. **Retrieval**: Start with top-5, adjust based on needs\n",
        "3. **Prompts**: Be explicit about using only context\n",
        "4. **Evaluation**: Test with diverse questions\n",
        "5. **Thresholds**: Set confidence thresholds for production\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "In Session 3, we'll learn:\n",
        "- **Reranking** for better retrieval precision\n",
        "- **Hybrid search** combining semantic + keyword search\n",
        "- **Proper evaluation** methods (LLM-as-judge)\n",
        "- **Why cosine similarity fails** for answer evaluation\n",
        "- **Production optimization** techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-34",
      "metadata": {
        "id": "cell-34"
      },
      "source": [
        "## Exercises (Optional)\n",
        "\n",
        "Try these to deepen your understanding:\n",
        "\n",
        "1. **Add your own documents**: Load PDFs or text files and index them\n",
        "2. **Experiment with chunk sizes**: Try 128, 256, 512, 1024 - which works best?\n",
        "3. **Test different embedding models**: Compare text-embedding-3-small vs large\n",
        "4. **Implement semantic chunking**: Split on section headers or topics\n",
        "5. **Add source citations**: Format answers with clickable source links\n",
        "6. **Build a comparison tool**: Evaluate multiple questions systematically\n",
        "7. **Try different LLMs**: Compare GPT-4o-mini vs other models for generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-35",
      "metadata": {
        "id": "cell-35"
      },
      "outputs": [],
      "source": [
        "# Your experiments here!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}