{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Levara/carnet--workshop-notebooks/blob/master/04_rag_experimentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygu_EkZjdR6J"
      },
      "source": [
        "# Session 2.4: RAG Experimentation Framework\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook is designed for **rapid experimentation** with RAG systems. Unlike notebook 03 which focuses on learning, this notebook lets you:\n",
        "\n",
        "- ‚úÖ Quickly test different chunk sizes\n",
        "- ‚úÖ Compare different embedding/LLM models\n",
        "- ‚úÖ Experiment with your own documents\n",
        "- ‚úÖ Run batch comparisons\n",
        "- ‚úÖ Configure everything from the bottom cells (no scrolling!)\n",
        "\n",
        "**Quick Start:**\n",
        "1. Run setup cells (1-3)\n",
        "2. Upload your documents (see instructions below)\n",
        "3. Jump to any experiment section at the bottom\n",
        "4. All configuration is done via function parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv7fBP5_dR6K"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpsdtF-ddR6K"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install chromadb openai pypdf2 requests tabulate -q\n",
        "\n",
        "print(\"‚úì Packages installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9guuV52dR6L"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import json\n",
        "from typing import List, Dict, Optional, Tuple, Any\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import PyPDF2\n",
        "import re\n",
        "from pathlib import Path\n",
        "from tabulate import tabulate\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚úì Imports loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBj_0zYadR6M"
      },
      "outputs": [],
      "source": [
        "# Configure OpenRouter API\n",
        "from google.colab import userdata\n",
        "os.environ['OPENROUTER_API_KEY'] = userdata.get('OPENROUTER_API_KEY')\n",
        "\n",
        "OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY', None)\n",
        "\n",
        "if not OPENROUTER_API_KEY:\n",
        "    raise ValueError(\"Please set your OpenRouter API key in Colab secrets.\")\n",
        "\n",
        "print(\"‚úì OpenRouter API configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaU39MTIdR6M"
      },
      "source": [
        "## 2. Document Upload Instructions\n",
        "\n",
        "### How to Upload Your Documents to Google Colab\n",
        "\n",
        "You have **3 documents** that make up your knowledge base. Follow these steps:\n",
        "\n",
        "#### **Method 1: Manual Upload (Recommended for small files)**\n",
        "\n",
        "1. **Click the folder icon** üìÅ in the left sidebar of Colab\n",
        "2. **Click the upload button** ‚¨ÜÔ∏è (looks like a file with an arrow)\n",
        "3. **Select your 3 documents** (PDF, TXT, or other text files)\n",
        "4. **Wait for upload to complete** - you'll see the files appear in the file browser\n",
        "5. **Note the file paths** - they'll be in `/content/filename.pdf` format\n",
        "\n",
        "#### **Method 2: Mount Google Drive (Recommended for larger files or permanent storage)**\n",
        "\n",
        "1. **Run the cell below** to mount your Google Drive\n",
        "2. **Upload your documents** to a folder in Google Drive (e.g., `My Drive/RAG_Workshop/`)\n",
        "3. **Access them** using paths like `/content/drive/MyDrive/RAG_Workshop/document.pdf`\n",
        "\n",
        "```python\n",
        "# Uncomment and run this to mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "#### **Method 3: Direct Upload in Code**\n",
        "\n",
        "Run the cell below to get an interactive file upload button:\n",
        "\n",
        "```python\n",
        "# Uncomment and run this to upload files via code\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# print(f\"Uploaded files: {list(uploaded.keys())}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### After Uploading\n",
        "\n",
        "Once your files are uploaded, you'll specify their paths when running experiments:\n",
        "\n",
        "```python\n",
        "document_paths = [\n",
        "    \"/content/document1.pdf\",\n",
        "    \"/content/document2.pdf\",\n",
        "    \"/content/document3.txt\"\n",
        "]\n",
        "```\n",
        "\n",
        "**‚ö†Ô∏è Note**: Files uploaded directly to `/content/` are **temporary** and will be deleted when the runtime restarts. Use Google Drive for permanent storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5479wAwSdR6M"
      },
      "source": [
        "## 3. Core RAG Components\n",
        "\n",
        "These are reusable classes and functions. **You don't need to modify these** - just run them once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FybDEMd6dR6N"
      },
      "outputs": [],
      "source": [
        "class OpenRouterEmbeddingFunction:\n",
        "    \"\"\"\n",
        "    Custom embedding function for ChromaDB using OpenRouter.\n",
        "    \"\"\"\n",
        "    def __init__(self, api_key: str, model: str):\n",
        "        self.client = OpenAI(\n",
        "            base_url=\"https://openrouter.ai/api/v1\",\n",
        "            api_key=api_key\n",
        "        )\n",
        "        self.model = model\n",
        "\n",
        "    def __call__(self, input: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for documents.\"\"\"\n",
        "        response = self.client.embeddings.create(\n",
        "            input=input,\n",
        "            model=self.model\n",
        "        )\n",
        "        return [item.embedding for item in response.data]\n",
        "\n",
        "    def embed_query(self, input: str) -> List[List[float]]:\n",
        "        \"\"\"Generate embedding for a single query.\"\"\"\n",
        "        response = self.client.embeddings.create(\n",
        "            input=input,\n",
        "            model=self.model\n",
        "        )\n",
        "        return [response.data[0].embedding]\n",
        "\n",
        "\n",
        "def load_pdf(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load PDF and extract text.\"\"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        full_text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            full_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        return {\n",
        "            \"source\": file_path,\n",
        "            \"full_text\": full_text,\n",
        "            \"num_pages\": len(pdf_reader.pages)\n",
        "        }\n",
        "\n",
        "\n",
        "def load_text_file(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load plain text file.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return {\n",
        "            \"source\": file_path,\n",
        "            \"full_text\": file.read(),\n",
        "            \"num_pages\": 1\n",
        "        }\n",
        "\n",
        "\n",
        "def load_document(file_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Load document (auto-detect PDF or text).\"\"\"\n",
        "    if file_path.lower().endswith('.pdf'):\n",
        "        return load_pdf(file_path)\n",
        "    else:\n",
        "        return load_text_file(file_path)\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean extracted text.\"\"\"\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def chunk_text_fixed(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "    \"\"\"Chunk text using fixed word count with overlap.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        if len(chunk.strip()) > 0:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "print(\"‚úì Core functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKuOAiSddR6N"
      },
      "outputs": [],
      "source": [
        "class RAGExperiment:\n",
        "    \"\"\"\n",
        "    Complete RAG system for experimentation.\n",
        "    All configuration is done via constructor parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        collection_name: str = \"rag_experiment\",\n",
        "        embedding_model: str = \"openai/text-embedding-3-small\",\n",
        "        llm_model: str = \"openai/gpt-4o-mini\",\n",
        "        chunk_size: int = 500,\n",
        "        overlap: int = 50,\n",
        "        top_k: int = 5,\n",
        "        temperature: float = 0.3,\n",
        "        api_key: str = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize RAG experiment.\n",
        "\n",
        "        Args:\n",
        "            collection_name: ChromaDB collection name\n",
        "            embedding_model: Model for embeddings (e.g., 'openai/text-embedding-3-small')\n",
        "            llm_model: Model for generation (e.g., 'openai/gpt-4o-mini')\n",
        "            chunk_size: Number of words per chunk\n",
        "            overlap: Number of overlapping words\n",
        "            top_k: Number of chunks to retrieve\n",
        "            temperature: LLM temperature\n",
        "            api_key: OpenRouter API key (uses env var if None)\n",
        "        \"\"\"\n",
        "        self.collection_name = collection_name\n",
        "        self.embedding_model = embedding_model\n",
        "        self.llm_model = llm_model\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "        self.top_k = top_k\n",
        "        self.temperature = temperature\n",
        "\n",
        "        self.api_key = api_key or os.getenv('OPENROUTER_API_KEY')\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"API key required\")\n",
        "\n",
        "        # Initialize clients\n",
        "        self.llm_client = OpenAI(\n",
        "            base_url=\"https://openrouter.ai/api/v1\",\n",
        "            api_key=self.api_key\n",
        "        )\n",
        "\n",
        "        self.embedding_function = OpenRouterEmbeddingFunction(\n",
        "            api_key=self.api_key,\n",
        "            model=self.embedding_model\n",
        "        )\n",
        "\n",
        "        # ChromaDB client and collection (will be set during indexing)\n",
        "        self.chroma_client = None\n",
        "        self.collection = None\n",
        "\n",
        "    def index_documents(\n",
        "        self,\n",
        "        document_paths: List[str],\n",
        "        recreate: bool = True,\n",
        "        verbose: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Load and index documents.\n",
        "\n",
        "        Args:\n",
        "            document_paths: List of file paths to documents\n",
        "            recreate: If True, delete existing collection and recreate\n",
        "            verbose: Print progress\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with indexing statistics\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(\"INDEXING DOCUMENTS\")\n",
        "            print(f\"{'='*80}\")\n",
        "            print(f\"Collection: {self.collection_name}\")\n",
        "            print(f\"Embedding model: {self.embedding_model}\")\n",
        "            print(f\"Chunk size: {self.chunk_size} words\")\n",
        "            print(f\"Overlap: {self.overlap} words\")\n",
        "            print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # Initialize ChromaDB\n",
        "        self.chroma_client = chromadb.Client()\n",
        "\n",
        "        # Delete existing collection if requested\n",
        "        if recreate:\n",
        "            try:\n",
        "                self.chroma_client.delete_collection(name=self.collection_name)\n",
        "                if verbose:\n",
        "                    print(f\"‚úì Deleted existing collection '{self.collection_name}'\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Create collection with cosine similarity\n",
        "        self.collection = self.chroma_client.create_collection(\n",
        "            name=self.collection_name,\n",
        "            embedding_function=self.embedding_function,\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "\n",
        "        # Process documents\n",
        "        all_chunks = []\n",
        "        all_ids = []\n",
        "        all_metadata = []\n",
        "\n",
        "        for doc_idx, doc_path in enumerate(document_paths):\n",
        "            if verbose:\n",
        "                print(f\"Processing: {Path(doc_path).name}\")\n",
        "\n",
        "            # Load document\n",
        "            doc = load_document(doc_path)\n",
        "\n",
        "            # Clean and chunk\n",
        "            cleaned_text = clean_text(doc['full_text'])\n",
        "            chunks = chunk_text_fixed(cleaned_text, self.chunk_size, self.overlap)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"  Created {len(chunks)} chunks\")\n",
        "\n",
        "            # Add to lists\n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                all_chunks.append(chunk)\n",
        "                all_ids.append(f\"doc{doc_idx}_chunk{chunk_idx}\")\n",
        "                all_metadata.append({\n",
        "                    \"source\": Path(doc_path).name,\n",
        "                    \"full_path\": doc_path,\n",
        "                    \"chunk_index\": chunk_idx,\n",
        "                    \"total_chunks\": len(chunks)\n",
        "                })\n",
        "\n",
        "        # Add to ChromaDB\n",
        "        if verbose:\n",
        "            print(f\"\\nAdding {len(all_chunks)} chunks to ChromaDB...\")\n",
        "\n",
        "        self.collection.add(\n",
        "            documents=all_chunks,\n",
        "            ids=all_ids,\n",
        "            metadatas=all_metadata\n",
        "        )\n",
        "\n",
        "        stats = {\n",
        "            \"num_documents\": len(document_paths),\n",
        "            \"num_chunks\": len(all_chunks),\n",
        "            \"chunk_size\": self.chunk_size,\n",
        "            \"overlap\": self.overlap\n",
        "        }\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n‚úì Indexing complete!\")\n",
        "            print(f\"  Documents indexed: {stats['num_documents']}\")\n",
        "            print(f\"  Total chunks: {stats['num_chunks']}\")\n",
        "            print(f\"{'='*80}\\n\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def query(\n",
        "        self,\n",
        "        question: str,\n",
        "        top_k: int = None,\n",
        "        temperature: float = None,\n",
        "        show_sources: bool = False,\n",
        "        language: str = \"hr\"\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Query the RAG system.\n",
        "\n",
        "        Args:\n",
        "            question: User question\n",
        "            top_k: Number of chunks to retrieve (uses default if None)\n",
        "            temperature: LLM temperature (uses default if None)\n",
        "            show_sources: Print retrieved sources\n",
        "            language: Response language ('hr' or 'en')\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with answer and metadata\n",
        "        \"\"\"\n",
        "        if self.collection is None:\n",
        "            raise ValueError(\"No documents indexed. Call index_documents() first.\")\n",
        "\n",
        "        top_k = top_k or self.top_k\n",
        "        temperature = temperature or self.temperature\n",
        "\n",
        "        # Retrieve chunks\n",
        "        results = self.collection.query(\n",
        "            query_texts=[question],\n",
        "            n_results=top_k\n",
        "        )\n",
        "\n",
        "        retrieved_chunks = results['documents'][0]\n",
        "        metadatas = results['metadatas'][0]\n",
        "        distances = results['distances'][0]\n",
        "        similarities = [1 - d for d in distances]\n",
        "\n",
        "        if show_sources:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(\"RETRIEVED SOURCES\")\n",
        "            print(f\"{'='*80}\")\n",
        "            for i, (chunk, meta, sim) in enumerate(zip(retrieved_chunks, metadatas, similarities), 1):\n",
        "                print(f\"\\n[{i}] Similarity: {sim:.3f} | Source: {meta['source']}\")\n",
        "                print(f\"    {chunk[:200]}...\")\n",
        "            print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # Construct prompt\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"[Dokument {i+1}]: {chunk}\"\n",
        "            for i, chunk in enumerate(retrieved_chunks)\n",
        "        ])\n",
        "\n",
        "        if language == \"hr\":\n",
        "            prompt = f\"\"\"Ti si pametan asistent koji odgovara na pitanja na temelju dostavljenog konteksta.\n",
        "\n",
        "Kontekst:\n",
        "{context}\n",
        "\n",
        "Pitanje: {question}\n",
        "\n",
        "Upute:\n",
        "- Koristi SAMO informacije iz dostavljenog konteksta\n",
        "- Ako odgovor nije u kontekstu, reci \"Nemam tu informaciju u dostavljenim dokumentima\"\n",
        "- Budi koncizan ali potpun u odgovoru\n",
        "- Odgovaraj na hrvatskom jeziku\n",
        "\n",
        "Odgovor:\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"You are a helpful assistant that answers questions based on the provided context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Instructions:\n",
        "- Use ONLY information from the provided context\n",
        "- If the answer is not in the context, say \"I don't have this information in the provided documents\"\n",
        "- Be concise but complete in your answer\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Generate answer\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.llm_model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "\n",
        "        answer = response.choices[0].message.content\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"sources\": metadatas,\n",
        "            \"retrieved_chunks\": retrieved_chunks,\n",
        "            \"similarities\": similarities,\n",
        "            \"top_k\": top_k,\n",
        "            \"temperature\": temperature\n",
        "        }\n",
        "\n",
        "    def get_config(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current configuration.\"\"\"\n",
        "        return {\n",
        "            \"collection_name\": self.collection_name,\n",
        "            \"embedding_model\": self.embedding_model,\n",
        "            \"llm_model\": self.llm_model,\n",
        "            \"chunk_size\": self.chunk_size,\n",
        "            \"overlap\": self.overlap,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"temperature\": self.temperature\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"‚úì RAGExperiment class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsiPGQb5dR6O"
      },
      "source": [
        "## 4. Comparison and Evaluation Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIcCc0mKdR6O"
      },
      "outputs": [],
      "source": [
        "def compare_configurations(\n",
        "    document_paths: List[str],\n",
        "    test_questions: List[str],\n",
        "    configs: List[Dict[str, Any]],\n",
        "    language: str = \"hr\"\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Compare multiple RAG configurations on the same questions.\n",
        "\n",
        "    Args:\n",
        "        document_paths: List of document paths to index\n",
        "        test_questions: List of questions to test\n",
        "        configs: List of configuration dicts (each with chunk_size, overlap, top_k, etc.)\n",
        "        language: Response language\n",
        "\n",
        "    Returns:\n",
        "        List of result dictionaries\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for i, config in enumerate(configs, 1):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"CONFIGURATION {i}/{len(configs)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Config: {config}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        # Create experiment with this config\n",
        "        exp = RAGExperiment(**config)\n",
        "\n",
        "        # Index documents\n",
        "        exp.index_documents(document_paths, verbose=True)\n",
        "\n",
        "        # Test all questions\n",
        "        config_results = {\n",
        "            \"config\": config,\n",
        "            \"questions\": []\n",
        "        }\n",
        "\n",
        "        for q_idx, question in enumerate(test_questions, 1):\n",
        "            print(f\"\\nQuestion {q_idx}/{len(test_questions)}: {question}\")\n",
        "            result = exp.query(question, show_sources=False, language=language)\n",
        "\n",
        "            config_results[\"questions\"].append({\n",
        "                \"question\": question,\n",
        "                \"answer\": result[\"answer\"],\n",
        "                \"avg_similarity\": sum(result[\"similarities\"]) / len(result[\"similarities\"]),\n",
        "                \"max_similarity\": max(result[\"similarities\"]),\n",
        "                \"num_sources\": len(result[\"sources\"])\n",
        "            })\n",
        "\n",
        "            print(f\"  Answer: {result['answer'][:150]}...\")\n",
        "            print(f\"  Avg similarity: {config_results['questions'][-1]['avg_similarity']:.3f}\")\n",
        "\n",
        "        results.append(config_results)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def print_comparison_table(comparison_results: List[Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Print a comparison table of results.\n",
        "\n",
        "    Args:\n",
        "        comparison_results: Results from compare_configurations()\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"COMPARISON SUMMARY\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # Create table data\n",
        "    headers = [\"Config\", \"Chunk Size\", \"Overlap\", \"Top-K\", \"Avg Similarity\", \"Answer Length\"]\n",
        "    rows = []\n",
        "\n",
        "    for i, result in enumerate(comparison_results, 1):\n",
        "        config = result[\"config\"]\n",
        "        avg_sim = sum(q[\"avg_similarity\"] for q in result[\"questions\"]) / len(result[\"questions\"])\n",
        "        avg_length = sum(len(q[\"answer\"]) for q in result[\"questions\"]) / len(result[\"questions\"])\n",
        "\n",
        "        rows.append([\n",
        "            f\"Config {i}\",\n",
        "            config.get(\"chunk_size\", \"N/A\"),\n",
        "            config.get(\"overlap\", \"N/A\"),\n",
        "            config.get(\"top_k\", \"N/A\"),\n",
        "            f\"{avg_sim:.3f}\",\n",
        "            f\"{avg_length:.0f} chars\"\n",
        "        ])\n",
        "\n",
        "    print(tabulate(rows, headers=headers, tablefmt=\"grid\"))\n",
        "    print()\n",
        "\n",
        "\n",
        "def export_results(results: List[Dict[str, Any]], filename: str = \"rag_results.json\"):\n",
        "    \"\"\"\n",
        "    Export results to JSON file.\n",
        "\n",
        "    Args:\n",
        "        results: Results from compare_configurations()\n",
        "        filename: Output filename\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"‚úì Results exported to {filename}\")\n",
        "\n",
        "\n",
        "print(\"‚úì Comparison utilities defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WOTeWdndR6O"
      },
      "source": [
        "---\n",
        "\n",
        "# EXPERIMENTS START HERE\n",
        "\n",
        "## Everything below can be run independently with full configuration!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoCQ2XlOdR6P"
      },
      "source": [
        "## Experiment 1: Quick Single Query Test\n",
        "\n",
        "**Test a single question with custom configuration.**\n",
        "\n",
        "All configuration is done here - no need to scroll up!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8wd_jAfdR6P"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURE EVERYTHING HERE\n",
        "# ============================================================================\n",
        "\n",
        "# Your document paths (update these after uploading)\n",
        "DOCUMENT_PATHS = [\n",
        "    \"/content/loomen_faq.md\",\n",
        "    \"/content/moodle.md\",\n",
        "    \"/content/tecaj.md\"\n",
        "]\n",
        "\n",
        "# RAG configuration\n",
        "EXPERIMENT_CONFIG = {\n",
        "    \"collection_name\": \"quick_test\",\n",
        "    \"embedding_model\": \"openai/text-embedding-3-small\",\n",
        "    \"llm_model\": \"openai/gpt-4o-mini\",\n",
        "    \"chunk_size\": 500,\n",
        "    \"overlap\": 50,\n",
        "    \"top_k\": 5,\n",
        "    \"temperature\": 0.3\n",
        "}\n",
        "\n",
        "# Your question\n",
        "QUESTION = \"≈†to je RAG sustav?\"  # Change this to your question\n",
        "\n",
        "# Language for response (\"hr\" or \"en\")\n",
        "LANGUAGE = \"hr\"\n",
        "\n",
        "# ============================================================================\n",
        "# RUN EXPERIMENT\n",
        "# ============================================================================\n",
        "\n",
        "# Create experiment\n",
        "exp = RAGExperiment(**EXPERIMENT_CONFIG)\n",
        "\n",
        "# Index documents\n",
        "exp.index_documents(DOCUMENT_PATHS, verbose=True)\n",
        "\n",
        "# Query\n",
        "result = exp.query(QUESTION, show_sources=True, language=LANGUAGE)\n",
        "\n",
        "# Display answer\n",
        "def display_answer(result):\n",
        "  print(f\"\\n{'='*80}\")\n",
        "  print(\"ANSWER\")\n",
        "  print(f\"{'='*80}\\n\")\n",
        "  print(result[\"answer\"])\n",
        "  print(f\"\\n{'='*80}\")\n",
        "  print(f\"Avg similarity: {sum(result['similarities']) / len(result['similarities']):.3f}\")\n",
        "  print(f\"Max similarity: {max(result['similarities']):.3f}\")\n",
        "  print(f\"{'='*80}\\n\")\n",
        "\n",
        "display_answer(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try out with your own questions\n",
        "\n",
        "Just change the `question` variable and re-run the cell again."
      ],
      "metadata": {
        "id": "gcKUyAt9QOOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your own query\n",
        "question = \"U kojim sluƒçajevima je potrebno direktno kontaktirati administratore Loomen-a?\"\n",
        "top_k = 2\n",
        "\n",
        "\n",
        "result  = exp.query(question, show_sources=True, language=\"hr\", top_k=top_k)\n",
        "display_answer(result)"
      ],
      "metadata": {
        "id": "v39uEjqmPyu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwJcDpjRdR6P"
      },
      "source": [
        "## Experiment 2: Compare Different Chunk Sizes\n",
        "\n",
        "**Test how chunk size affects retrieval quality.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3t2ky9WdR6P"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURE EVERYTHING HERE\n",
        "# ============================================================================\n",
        "# Hard questions json\n",
        "HARD_QUESTIONS_FILENAME = \"/content/loomen_faq.hard_questions.json\"\n",
        "# Load json from content folder\n",
        "with open(HARD_QUESTIONS_FILENAME, 'r') as f:\n",
        "    HARD_QUESTIONS_DATA = json.load(f)\n",
        "\n",
        "HARD_QUESTIONS = [ x[\"question\"] for x in HARD_QUESTIONS_DATA ]\n",
        "\n",
        "\n",
        "# Your document paths\n",
        "DOCUMENT_PATHS = [\n",
        "    \"/content/loomen_faq.md\",\n",
        "    \"/content/moodle.md\",\n",
        "    \"/content/tecaj.md\"\n",
        "]\n",
        "\n",
        "# Test questions\n",
        "TEST_QUESTIONS = [\n",
        "    \"≈†to je RAG?\",\n",
        "    \"Kako funkcionira pretra≈æivanje?\",\n",
        "    \"Koje su prednosti ovog sustava?\",\n",
        "    # Extend with your questions\n",
        "]\n",
        "TEST_QUESTIONS += HARD_QUESTIONS\n",
        "\n",
        "# Chunk sizes to compare\n",
        "CHUNK_SIZES = [256, 512, 1024]\n",
        "\n",
        "# Fixed parameters\n",
        "BASE_CONFIG = {\n",
        "    \"embedding_model\": \"openai/text-embedding-3-small\",\n",
        "    \"llm_model\": \"openai/gpt-4o-mini\",\n",
        "    \"overlap\": 50,\n",
        "    \"top_k\": 5,\n",
        "    \"temperature\": 0.3\n",
        "}\n",
        "\n",
        "LANGUAGE = \"hr\"\n",
        "\n",
        "# ============================================================================\n",
        "# RUN EXPERIMENT\n",
        "# ============================================================================\n",
        "\n",
        "# Create configs for each chunk size\n",
        "configs = [\n",
        "    {**BASE_CONFIG, \"collection_name\": f\"chunk_{size}\", \"chunk_size\": size}\n",
        "    for size in CHUNK_SIZES\n",
        "]\n",
        "\n",
        "# Run comparison\n",
        "results = compare_configurations(\n",
        "    document_paths=DOCUMENT_PATHS,\n",
        "    test_questions=TEST_QUESTIONS,\n",
        "    configs=configs,\n",
        "    language=LANGUAGE\n",
        ")\n",
        "\n",
        "# Print summary table\n",
        "print_comparison_table(results)\n",
        "\n",
        "# Export results\n",
        "export_results(results, \"chunk_size_comparison.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SQXkKNJdR6P"
      },
      "source": [
        "## Experiment 3: Compare Different Models\n",
        "\n",
        "**Test different embedding or LLM models.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPc5O3wpdR6P"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURE EVERYTHING HERE\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# CONFIGURE EVERYTHING HERE\n",
        "# ============================================================================\n",
        "# Hard questions json\n",
        "HARD_QUESTIONS_FILENAME = \"/content/loomen_faq.hard_questions.json\"\n",
        "# Load json from content folder\n",
        "with open(HARD_QUESTIONS_FILENAME, 'r') as f:\n",
        "    HARD_QUESTIONS_DATA = json.load(f)\n",
        "\n",
        "HARD_QUESTIONS = [ x[\"question\"] for x in HARD_QUESTIONS_DATA ]\n",
        "\n",
        "\n",
        "# Your document paths\n",
        "DOCUMENT_PATHS = [\n",
        "    \"/content/loomen_faq.md\",\n",
        "    \"/content/moodle.md\",\n",
        "    \"/content/tecaj.md\"\n",
        "]\n",
        "\n",
        "# Test questions\n",
        "TEST_QUESTIONS = [\n",
        "    \"≈†to je RAG?\",\n",
        "    \"Kako funkcionira pretra≈æivanje?\",\n",
        "    \"Koje su prednosti ovog sustava?\",\n",
        "    # Extend with your questions\n",
        "]\n",
        "TEST_QUESTIONS += HARD_QUESTIONS\n",
        "\n",
        "\n",
        "# Models to compare\n",
        "EMBEDDING_MODELS = [\n",
        "    #\"openai/text-embedding-3-small\",\n",
        "    \"qwen/qwen3-embedding-8b\"\n",
        "\n",
        "]\n",
        "\n",
        "# Or compare LLM models:\n",
        "# LLM_MODELS = [\n",
        "#     \"openai/gpt-4o-mini\",\n",
        "#     \"openai/gpt-4o\",\n",
        "#     \"anthropic/claude-3.5-sonnet\"\n",
        "# ]\n",
        "\n",
        "# Fixed parameters\n",
        "BASE_CONFIG = {\n",
        "    \"llm_model\": \"openai/gpt-4o-mini\",\n",
        "    \"chunk_size\": 250,\n",
        "    \"overlap\": 50,\n",
        "    \"top_k\": 5,\n",
        "    \"temperature\": 0.3\n",
        "}\n",
        "\n",
        "LANGUAGE = \"hr\"\n",
        "\n",
        "# ============================================================================\n",
        "# RUN EXPERIMENT\n",
        "# ============================================================================\n",
        "\n",
        "# Create configs for each embedding model\n",
        "configs = [\n",
        "    {**BASE_CONFIG, \"collection_name\": f\"embed_{i}\", \"embedding_model\": model}\n",
        "    for i, model in enumerate(EMBEDDING_MODELS)\n",
        "]\n",
        "\n",
        "# Or for LLM models:\n",
        "# configs = [\n",
        "#     {**BASE_CONFIG, \"collection_name\": f\"llm_{i}\", \"llm_model\": model}\n",
        "#     for i, model in enumerate(LLM_MODELS)\n",
        "# ]\n",
        "\n",
        "# Run comparison\n",
        "results = compare_configurations(\n",
        "    document_paths=DOCUMENT_PATHS,\n",
        "    test_questions=TEST_QUESTIONS,\n",
        "    configs=configs,\n",
        "    language=LANGUAGE\n",
        ")\n",
        "\n",
        "# Print summary\n",
        "print_comparison_table(results)\n",
        "export_results(results, \"model_comparison.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwlxehetdR6Q"
      },
      "source": [
        "## Experiment 4: Top-K Comparison\n",
        "\n",
        "**Test different numbers of retrieved chunks.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9-zQAODdR6Q"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURE EVERYTHING HERE\n",
        "# ============================================================================\n",
        "\n",
        "# Your document paths\n",
        "DOCUMENT_PATHS = [\n",
        "    \"/content/loomen_faq.md\",\n",
        "    \"/content/moodle.md\",\n",
        "    \"/content/tecaj.md\"\n",
        "]\n",
        "\n",
        "TEST_QUESTIONS = [\n",
        "    \"Koja je glavna tema dokumenta?\",\n",
        "    \"Objasni kljuƒçne koncepte.\"\n",
        "]\n",
        "\n",
        "# Top-K values to compare\n",
        "TOP_K_VALUES = [3, 5, 10]\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"embedding_model\": \"openai/text-embedding-3-small\",\n",
        "    \"llm_model\": \"openai/gpt-4o-mini\",\n",
        "    \"chunk_size\": 500,\n",
        "    \"overlap\": 50,\n",
        "    \"temperature\": 0.3\n",
        "}\n",
        "\n",
        "LANGUAGE = \"hr\"\n",
        "\n",
        "# ============================================================================\n",
        "# RUN EXPERIMENT\n",
        "# ============================================================================\n",
        "\n",
        "configs = [\n",
        "    {**BASE_CONFIG, \"collection_name\": f\"topk_{k}\", \"top_k\": k}\n",
        "    for k in TOP_K_VALUES\n",
        "]\n",
        "\n",
        "results = compare_configurations(\n",
        "    document_paths=DOCUMENT_PATHS,\n",
        "    test_questions=TEST_QUESTIONS,\n",
        "    configs=configs,\n",
        "    language=LANGUAGE\n",
        ")\n",
        "\n",
        "print_comparison_table(results)\n",
        "export_results(results, \"topk_comparison.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq5vjlXtdR6Q"
      },
      "source": [
        "## Experiment 5: Custom Batch Testing\n",
        "\n",
        "**Create your own custom experiment with any parameters.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5dQ_-UBdR6Q"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FULLY CUSTOMIZABLE EXPERIMENT\n",
        "# ============================================================================\n",
        "\n",
        "DOCUMENT_PATHS = [\n",
        "    \"/content/document1.pdf\",\n",
        "    \"/content/document2.pdf\",\n",
        "    \"/content/document3.txt\"\n",
        "]\n",
        "\n",
        "TEST_QUESTIONS = [\n",
        "    \"Your question 1?\",\n",
        "    \"Your question 2?\",\n",
        "    \"Your question 3?\"\n",
        "]\n",
        "\n",
        "# Define your own configurations to compare\n",
        "CUSTOM_CONFIGS = [\n",
        "    {\n",
        "        \"collection_name\": \"config_a\",\n",
        "        \"embedding_model\": \"openai/text-embedding-3-small\",\n",
        "        \"llm_model\": \"openai/gpt-4o-mini\",\n",
        "        \"chunk_size\": 256,\n",
        "        \"overlap\": 30,\n",
        "        \"top_k\": 3,\n",
        "        \"temperature\": 0.2\n",
        "    },\n",
        "    {\n",
        "        \"collection_name\": \"config_b\",\n",
        "        \"embedding_model\": \"openai/text-embedding-3-small\",\n",
        "        \"llm_model\": \"openai/gpt-4o-mini\",\n",
        "        \"chunk_size\": 512,\n",
        "        \"overlap\": 50,\n",
        "        \"top_k\": 5,\n",
        "        \"temperature\": 0.3\n",
        "    },\n",
        "    # Add more configurations as needed\n",
        "]\n",
        "\n",
        "LANGUAGE = \"hr\"\n",
        "\n",
        "# ============================================================================\n",
        "# RUN EXPERIMENT\n",
        "# ============================================================================\n",
        "\n",
        "results = compare_configurations(\n",
        "    document_paths=DOCUMENT_PATHS,\n",
        "    test_questions=TEST_QUESTIONS,\n",
        "    configs=CUSTOM_CONFIGS,\n",
        "    language=LANGUAGE\n",
        ")\n",
        "\n",
        "print_comparison_table(results)\n",
        "export_results(results, \"custom_experiment.json\")\n",
        "\n",
        "# Print detailed results for each question\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"DETAILED RESULTS\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"\\nConfiguration {i}:\")\n",
        "    print(f\"  {result['config']}\\n\")\n",
        "\n",
        "    for q_result in result[\"questions\"]:\n",
        "        print(f\"  Q: {q_result['question']}\")\n",
        "        print(f\"  A: {q_result['answer'][:200]}...\")\n",
        "        print(f\"  Similarity: {q_result['avg_similarity']:.3f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AHzOAgAdR6Q"
      },
      "source": [
        "## Interactive Playground\n",
        "\n",
        "**Quick testing area - modify and run repeatedly.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auv2SyOVdR6Q"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# QUICK PLAYGROUND - Run this cell multiple times with different questions\n",
        "# ============================================================================\n",
        "\n",
        "# First time: Set up the experiment (comment out after first run)\n",
        "playground_exp = RAGExperiment(\n",
        "    collection_name=\"playground\",\n",
        "    embedding_model=\"openai/text-embedding-3-small\",\n",
        "    llm_model=\"openai/gpt-4o-mini\",\n",
        "    chunk_size=500,\n",
        "    overlap=50,\n",
        "    top_k=5\n",
        ")\n",
        "\n",
        "playground_exp.index_documents([\n",
        "    \"/content/document1.pdf\",\n",
        "    \"/content/document2.pdf\",\n",
        "    \"/content/document3.txt\"\n",
        "], verbose=False)\n",
        "\n",
        "print(\"‚úì Playground ready!\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# Ask questions here - change and run repeatedly\n",
        "# ============================================================================\n",
        "\n",
        "question = \"Your question here?\"  # <-- CHANGE THIS\n",
        "\n",
        "result = playground_exp.query(question, show_sources=True, language=\"hr\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ANSWER\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "print(result[\"answer\"])\n",
        "print(f\"\\n{'='*80}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUGPuongdR6Q"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook provides a framework for rapid RAG experimentation:\n",
        "\n",
        "‚úÖ **No scrolling needed** - All config in each experiment cell  \n",
        "‚úÖ **Quick iterations** - Change parameters and re-run  \n",
        "‚úÖ **Easy comparisons** - Side-by-side evaluation  \n",
        "‚úÖ **Flexible** - Use your own documents  \n",
        "‚úÖ **Export results** - Save comparisons as JSON  \n",
        "\n",
        "### Tips for Experimentation\n",
        "\n",
        "1. **Start simple**: Test one variable at a time (chunk size, then top-k, then model)\n",
        "2. **Use consistent questions**: Same questions across experiments for fair comparison\n",
        "3. **Check similarities**: Low similarity scores mean irrelevant retrievals\n",
        "4. **Export results**: Keep track of what works best\n",
        "5. **Iterate quickly**: This notebook is designed for rapid testing\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Test with your own documents\n",
        "- Try different embedding models (text-embedding-3-large, multilingual models)\n",
        "- Experiment with different LLMs (Claude, GPT-4, etc.)\n",
        "- Test edge cases (questions not in documents)\n",
        "- Optimize for your specific use case"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}